{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Welcome to our website.</p>"},{"location":"checklist/","title":"Site Checklist","text":"<ol> <li>Customize the fields in your mkdocs.yml file</li> <li>Configure Google Analytics to use the right site ID</li> <li>Make sure that your .gitignore file includes the <code>site</code> directory</li> <li>Test the build</li> <li>Make sure the Edit button appears</li> <li>Make sure that code color heightening renders correctly</li> <li>run <code>git config advice.addIgnoredFile false</code></li> </ol>"},{"location":"code-highlight-test/","title":"Code Syntax Color Highlight Test","text":""},{"location":"code-highlight-test/#python","title":"Python","text":"<pre><code>hello_string = \"Hello World!\"\nprint(hello_string)\nx = 1\nif x == 1:\n    # indented four spaces\n    print(\"x is 1.\")\n</code></pre>"},{"location":"contact/","title":"Contact","text":"<p>Please contact me on LinkedIn</p> <p>Thanks! - Dan</p>"},{"location":"glossary/","title":"Glossary of Terms","text":""},{"location":"glossary/#iso-definition","title":"ISO Definition","text":"<p>A term definition is considered to be consistent with ISO metadata registry guideline 11179 if it meets the following criteria:</p> <ol> <li>Precise</li> <li>Concise</li> <li>Distinct</li> <li>Non-circular</li> <li>Unencumbered with business rules</li> </ol>"},{"location":"glossary/#term","title":"Term","text":"<p>This is the definition of the term.</p>"},{"location":"how-we-built-this-site/","title":"How We Built This Site","text":"<p>This page describes how we built this website and some of  the rationale behind why we made various design choices.</p>"},{"location":"how-we-built-this-site/#python","title":"Python","text":"<p>MicroSims are about how we use generative AI to create animations and simulations.  The language of AI is Python.  So we wanted to create a site that could be easily understood by Python developers.</p>"},{"location":"how-we-built-this-site/#mkdocs-vs-docusaurus","title":"Mkdocs vs. Docusaurus","text":"<p>There are two main tools used by Python developers to write documentation: Mkdocs and Docusaurus.  Mkdocs is easier to use and more popular than Docusaurus. Docusaurus is also optimized for single-page applications. Mkdocs also has an extensive library of themes and plugins. None of us are experts in JavaScript or React. Based on our ChatGPT Analysis of the Tradeoffs we chose mkdocs for this site management.</p>"},{"location":"how-we-built-this-site/#github-and-github-pages","title":"GitHub and GitHub Pages","text":"<p>GitHub is a logical choice to store our  site source code and documentation.  GitHub also has a Custom GitHub Action that does auto-deployment if any files on the site change. We don't currently have this action enabled, but other teams can use this feature if they don't have the ability to do a local build with mkdocs.</p> <p>GitHub also has Issues,  Projects and releases that we can use to manage our bugs and tasks.</p> <p>The best practice for low-cost websites that have public-only content is GitHub Pages. Mkdocs has a command (<code>mkdocs gh-deploy</code>) that does deployment directly to GitHub Pages.  This was an easy choice to make.</p>"},{"location":"how-we-built-this-site/#github-clone","title":"GitHub Clone","text":"<p>If you would like to clone this repository, here are the commands:</p> <pre><code>mkdir projects\ncd projects\ngit clone https://github.com/dmccreary/microsims\n</code></pre>"},{"location":"how-we-built-this-site/#after-changes","title":"After Changes","text":"<p>After you make local changes you must do the following:</p> <pre><code># add the new files to a a local commit transaction\ngit add FILES\n# Execute the a local commit with a message about what and why you are doing the commit\ngit commit -m \"comment\"\n# Update the central GitHub repository\ngit push\n</code></pre>"},{"location":"how-we-built-this-site/#material-theme","title":"Material Theme","text":"<p>We had several options when picking a mkdocs theme:</p> <ol> <li>Mkdocs default</li> <li>Readthedocs</li> <li>Third-Party Themes See Ranking</li> </ol> <p>The Material Theme had 16K stars.  No other theme had over a few hundred. This was also an easy design decision.</p> <p>One key criterial was the social Open Graph tags so that when our users post a link to a simulation, the image of the simulation is included in the link.  Since Material supported this, we used the Material theme. You can see our ChatGPT Design Decision Analysis if you want to check our decision process.</p>"},{"location":"how-we-built-this-site/#enable-edit-icon","title":"Enable Edit Icon","text":"<p>To enable the Edit icon on all pages, you must add the edit_uri and the content.action.edit under the theme features area.</p> <pre><code>edit_uri: edit/master/docs/\n</code></pre> <pre><code>    theme:\n        features:\n            - content.action.edit\n</code></pre>"},{"location":"how-we-built-this-site/#conda-vs-venv","title":"Conda vs VENV","text":"<p>There are two choices for virtual environments.  We can use the native Python venv or use Conda.  venv is simle but is only designed for pure Python projects.  We imagine that this site could use JavaScript and other langauges in the future, so we picked Conda. There is nothing on this microsite that prevents you from using one or the other.  See the ChatGPT Analysis Here.</p> <p>Here is the conda script that we ran to create a new mkdocs environment that also supports the material social imaging libraries.</p> <pre><code>conda deactivate\nconda create -n mkdocs python=3\nconda activate mkdocs\npip install mkdocs \"mkdocs-material[imaging]\"\n</code></pre>"},{"location":"how-we-built-this-site/#mkdocs-commands","title":"Mkdocs Commands","text":"<p>There are three simple mkdoc commands we use.</p>"},{"location":"how-we-built-this-site/#local-build","title":"Local Build","text":"<pre><code>mkdocs build\n</code></pre> <p>This builds your website in a folder called <code>site</code>.  Use this to test that the mkdocs.yml site is working and does not have any errors.</p>"},{"location":"how-we-built-this-site/#run-a-local-server","title":"Run a Local Server","text":"<pre><code>mkdocs serve\n</code></pre> <p>This runs a server on <code>http://localhost:8000</code>. Use this to test the display formatting locally before you push your code up to the GitHub repo.</p> <pre><code>mkdoc gh-deploy\n</code></pre> <p>This pushes everything up to the GitHub Pages site. Note that it does not commit your code to GitHub.</p>"},{"location":"how-we-built-this-site/#mkdocs-material-social-tags","title":"Mkdocs Material Social Tags","text":"<p>We are using the Material Social tags.  This is a work in progress!</p> <p>Here is what we have learned.</p> <ol> <li>There are extensive image processing libraries that can't be installed with just pip.  You will need to run a tool like brew on the Mac to get the libraries installed.</li> <li>Even after <code>brew</code> installs the libraries, you have to get your environment to find the libraries.  The only way I could get that to work was to set up a local UNIX environment variable.</li> </ol> <p>Here is the brew command that I ran:</p> <pre><code>brew install cairo freetype libffi libjpeg libpng zlib\n</code></pre> <p>I then had to add the following to my ~/.zshrc file:</p> <pre><code>export DYLD_FALLBACK_LIBRARY_PATH=/opt/homebrew/lib\n</code></pre> <p>Note that I am running on a Mac with Apple silicon.  This means that the image libraries that brew downloads must be specific to the Mac Arm instruction set.</p> <ul> <li>Cover images for blog post #4364</li> <li>Discussion on overriding the Social Card Image</li> </ul>"},{"location":"license/","title":"Creative Commons License","text":"<p>All content in this repository is governed by the following license agreement:</p>"},{"location":"license/#license-type","title":"License Type","text":"<p>Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0 DEED)</p>"},{"location":"license/#link-to-license-agreement","title":"Link to License Agreement","text":"<p>https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en</p>"},{"location":"license/#your-rights","title":"Your Rights","text":"<p>You are free to:</p> <ul> <li>Share \u2014 copy and redistribute the material in any medium or format</li> <li>Adapt \u2014 remix, transform, and build upon the material</li> </ul> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p>"},{"location":"license/#restrictions","title":"Restrictions","text":"<ul> <li>Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</li> <li>NonCommercial \u2014 You may not use the material for commercial purposes.</li> <li>ShareAlike \u2014 If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.</li> <li>No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.</li> </ul> <p>Notices</p> <p>You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.</p> <p>No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.</p> <p>This deed highlights only some of the key features and terms of the actual license. It is not a license and has no legal value. You should carefully review all of the terms and conditions of the actual license before using the licensed material.</p>"},{"location":"references/","title":"Site References","text":"<ol> <li>mkdocs - https://www.mkdocs.org/ - this is our tool for building the website.  It converts Markdown into HTML in the <code>site</code> directory.</li> <li>mkdocs material theme - https://squidfunk.github.io/mkdocs-material/ - this is the theme for our site.  The theme adds the user interface elements that give our site the look and feel.  It also has the features such as social cards.</li> <li>GitHub Pages - https://pages.github.com/ - this is the free tool for hosting public websites created by mkdocs</li> <li>Markdown - https://www.mkdocs.org/user-guide/writing-your-docs/#writing-with-markdown - this is the format we use for text.  It allows us to have headers, lists, tables, links and images without learning HTML.</li> <li>Deploy Mkdocs GitHub Action - https://github.com/marketplace/actions/deploy-mkdocs - this is the tool we use to automatically build our site after edits are checked in with Git.</li> <li>Git Book - https://git-scm.com/book/en/v2 - a useful book on Git.  Just read the first two chapters to learn how to check in new code.</li> <li>Conda - https://conda.io/ - this is a command line tool that keeps our Python libraries organized for each project.</li> <li>VS Code - https://code.visualstudio.com/ - this is the integrated development environment we use to mange the files on our website.</li> <li>Markdown Paste - https://marketplace.visualstudio.com/items?itemName=telesoho.vscode-markdown-paste-image - this is the VS code extension we use to make sure we keep the markdown format generated by ChatGPT.</li> </ol>"},{"location":"prompts/","title":"NLP Course Prompts","text":""},{"location":"prompts/01-course-outline/","title":"Old-School Rules-based NLP Course","text":"<p>Prompt</p> <p>Create an outline for a college level course on Natural Language processing with Python PyTorch. </p>"},{"location":"prompts/01-course-outline/#response","title":"Response","text":"<p>I'll create an outline for a college-level course on Natural Language Processing with Python, taking into account students' access to generative AI tools.</p>"},{"location":"prompts/01-course-outline/#natural-language-processing-with-python","title":"Natural Language Processing with Python","text":""},{"location":"prompts/01-course-outline/#course-description","title":"Course Description","text":"<p>This course explores the theory and application of Natural Language Processing (NLP) using Python. Students will learn fundamental NLP concepts, techniques, and tools while developing practical skills for text analysis, language understanding, and building NLP applications. The course integrates generative AI tools as learning aids and explores their impact on the NLP landscape.</p>"},{"location":"prompts/01-course-outline/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand core NLP concepts and techniques</li> <li>Develop proficiency in Python libraries for NLP</li> <li>Apply machine learning and deep learning methods to NLP tasks</li> <li>Create and evaluate NLP applications</li> <li>Critically assess the role of generative AI in NLP</li> <li>Develop ethical awareness of NLP applications and implications</li> </ul>"},{"location":"prompts/01-course-outline/#week-1-introduction-to-nlp","title":"Week 1: Introduction to NLP","text":"<ul> <li>Overview of NLP and its applications</li> <li>History and evolution of NLP</li> <li>Python ecosystem for NLP</li> <li>Setting up development environments</li> <li>Lab: Basic text processing with NLTK and spaCy</li> </ul>"},{"location":"prompts/01-course-outline/#week-2-text-preprocessing","title":"Week 2: Text Preprocessing","text":"<ul> <li>Tokenization techniques</li> <li>Stemming and lemmatization</li> <li>Stop word removal</li> <li>Regular expressions for text processing</li> <li>Lab: Building a text preprocessing pipeline</li> </ul>"},{"location":"prompts/01-course-outline/#week-3-text-representation","title":"Week 3: Text Representation","text":"<ul> <li>Bag of Words model</li> <li>TF-IDF vectorization</li> <li>Word embeddings: Word2Vec, GloVe, FastText</li> <li>Contextual embeddings</li> <li>Lab: Implementing different text representation methods</li> </ul>"},{"location":"prompts/01-course-outline/#week-4-language-modeling","title":"Week 4: Language Modeling","text":"<ul> <li>N-gram models</li> <li>Statistical language models</li> <li>Neural language models</li> <li>Evaluation metrics</li> <li>Lab: Building and evaluating language models</li> </ul>"},{"location":"prompts/01-course-outline/#week-5-text-classification","title":"Week 5: Text Classification","text":"<ul> <li>Supervised learning for text</li> <li>Feature engineering for text</li> <li>Naive Bayes, SVM, and logistic regression</li> <li>Neural networks for text classification</li> <li>Lab: Sentiment analysis application</li> </ul>"},{"location":"prompts/01-course-outline/#week-6-sequence-labeling","title":"Week 6: Sequence Labeling","text":"<ul> <li>Named Entity Recognition (NER)</li> <li>Part-of-Speech (POS) tagging</li> <li>Hidden Markov Models</li> <li>Conditional Random Fields</li> <li>Lab: Building a custom NER system</li> </ul>"},{"location":"prompts/01-course-outline/#week-7-generative-ai-for-nlp","title":"Week 7: Generative AI for NLP","text":"<ul> <li>Introduction to transformer architectures</li> <li>Pre-trained language models (BERT, GPT)</li> <li>Fine-tuning pre-trained models</li> <li>Lab: Fine-tuning a model for specific NLP task</li> </ul>"},{"location":"prompts/01-course-outline/#week-8-topic-modeling-and-information-extraction","title":"Week 8: Topic Modeling and Information Extraction","text":"<ul> <li>Latent Dirichlet Allocation (LDA)</li> <li>Non-negative Matrix Factorization</li> <li>Relationship extraction</li> <li>Knowledge graphs</li> <li>Lab: Topic modeling on real-world corpus</li> </ul>"},{"location":"prompts/01-course-outline/#week-9-machine-translation-and-sequence-to-sequence-models","title":"Week 9: Machine Translation and Sequence-to-Sequence Models","text":"<ul> <li>Statistical machine translation</li> <li>Neural machine translation</li> <li>Encoder-decoder architectures</li> <li>Attention mechanisms</li> <li>Lab: Building a simple translator</li> </ul>"},{"location":"prompts/01-course-outline/#week-10-dialogue-systems-and-chatbots","title":"Week 10: Dialogue Systems and Chatbots","text":"<ul> <li>Architectures for dialogue systems</li> <li>Intent recognition and slot filling</li> <li>Response generation</li> <li>Evaluation of dialogue systems</li> <li>Lab: Building a task-oriented chatbot</li> </ul>"},{"location":"prompts/01-course-outline/#week-11-nlp-with-deep-learning","title":"Week 11: NLP with Deep Learning","text":"<ul> <li>Recurrent Neural Networks (RNNs)</li> <li>Long Short-Term Memory (LSTM)</li> <li>Gated Recurrent Units (GRUs)</li> <li>Transformers in depth</li> <li>Lab: Implementing a deep learning model for text</li> </ul>"},{"location":"prompts/01-course-outline/#week-12-ethical-considerations-in-nlp","title":"Week 12: Ethical Considerations in NLP","text":"<ul> <li>Bias in language models</li> <li>Privacy concerns</li> <li>Ethical guidelines for NLP applications</li> <li>Responsible AI use cases</li> <li>Lab: Auditing models for bias</li> </ul>"},{"location":"prompts/01-course-outline/#week-13-advanced-applications-and-current-research","title":"Week 13: Advanced Applications and Current Research","text":"<ul> <li>Multimodal NLP</li> <li>Transfer learning in NLP</li> <li>Few-shot and zero-shot learning</li> <li>Latest research directions</li> <li>Lab: Exploring state-of-the-art models</li> </ul>"},{"location":"prompts/01-course-outline/#week-14-project-presentations-and-future-trends","title":"Week 14: Project Presentations and Future Trends","text":"<ul> <li>Student project presentations</li> <li>Industry applications of NLP</li> <li>Career paths in NLP</li> <li>Future directions and challenges</li> <li>Lab: Final project showcase</li> </ul>"},{"location":"prompts/01-course-outline/#assessment-methods","title":"Assessment Methods","text":"<ul> <li>Weekly coding assignments (30%)</li> <li>Midterm project: Building a practical NLP application (20%)</li> <li>Research paper analysis and presentation (15%)</li> <li>Final project: Novel NLP application development (25%)</li> <li>Class participation and discussions (10%)</li> </ul>"},{"location":"prompts/01-course-outline/#resources","title":"Resources","text":"<ul> <li>Primary textbook: \"Natural Language Processing with Python\" by Bird, Klein, and Loper</li> <li>Supplementary text: \"Speech and Language Processing\" by Jurafsky and Martin</li> <li>Online resources: Hugging Face documentation, Papers with Code, PyTorch tutorials</li> <li>Python libraries: NLTK, spaCy, scikit-learn, PyTorch, Hugging Face Transformers</li> </ul>"},{"location":"prompts/01-course-outline/#special-notes-on-generative-ai-use","title":"Special Notes on Generative AI Use","text":"<ul> <li>Generative AI tools may be used for code assistance, debugging, and generating ideas</li> <li>All work must include critical analysis of AI-generated content</li> <li>Assignments will include components that evaluate students' understanding beyond what generative AI can provide</li> <li>Students must cite and document all AI-assisted components of their work</li> <li>Some assessments will be designed as \"AI-free zones\" to evaluate independent skills</li> </ul>"},{"location":"prompts/02-course-description/","title":"02 course description","text":"<p>Here's a detailed course description for a college course catalog on Natural Language Processing with Python:</p>"},{"location":"prompts/02-course-description/#natural-language-processing-with-python","title":"Natural Language Processing with Python","text":"<p>Instructor: Dan McCreary Credits: 4 Prerequisites: Python, Data Structures, Machine Learning or instructor permission   </p>"},{"location":"prompts/02-course-description/#catalog-description","title":"Catalog Description","text":"<p>This course provides a comprehensive exploration of Natural Language Processing (NLP) theory and application using Python programming. Students will examine how computers process, analyze, and generate human language through computational and linguistic approaches. The course covers fundamental NLP techniques from traditional rule-based methods to modern deep learning approaches, with emphasis on practical implementation.</p> <p>Students will gain hands-on experience with industry-standard Python libraries including Pytorch. Topics include text preprocessing, language modeling, text classification, named entity recognition, sentiment analysis, machine translation, and dialogue systems. The course does not spend much time on traditional rules-based NLP techniques and puts a focus on contemporary generative AI systems.  We use generative AI tools to generate sample Python Pytorch code.</p> <p>Throughout the semester, students will develop increasingly sophisticated NLP applications, culminating in a final project that addresses a real-world language processing challenge. The ethical implications of NLP technologies, including bias, privacy concerns, and responsible deployment, will be integrated throughout the curriculum.</p>"},{"location":"prompts/02-course-description/#course-format","title":"Course Format","text":"<p>The course combines theoretical lectures with hands-on lab sessions. Each week includes one 110-minute lecture focusing on core concepts and one 110-minute lab session dedicated to implementation and problem-solving. Students will complete weekly programming assignments, a midterm project, a research presentation, and a comprehensive final project that demonstrates mastery of course concepts.</p> <p>Dan McCreary brings extensive industry experience in applied NLP, having led language processing initiatives at several technology firms before joining the faculty. Guest speakers from industry and research institutions will provide additional perspectives on current NLP applications and research frontiers.</p>"},{"location":"prompts/02-course-description/#note-on-generative-ai","title":"Note on Generative AI","text":"<p>This course acknowledges and incorporates the reality of generative AI tools in the computational landscape. Students are permitted and encouraged to use generative AI tools as learning aids, with appropriate documentation. However, assessments are designed to evaluate deep understanding beyond what generative tools alone can provide. Dan McCreary will provide clear guidelines for appropriate and inappropriate uses of AI assistance for each assignment.</p>"},{"location":"prompts/02-modern-course/","title":"Natural Language Processing with Python PyTorch: Course Outline","text":"<p>Prompt</p> <p>Create an outline for a modern college level course on Natural Language processing with Python PyTorch. Assume that students all have access to many generative AI tools and agentic IDEs to generate Python code.  Assume that we will use generative AI tools to generate a large amount of Python PyTorch code.</p> <p>Do not spend much time on traditional rules-based NLP programs and skim over topic such as identifying parts of speech, Stemming and lemmatization, Stop word removal, regular expressions for text processing, TF-IDF etc.</p> <p>Place a strong focus on using large-language-models (LLMs) to solve NLP problems.  Cover transformers and attention in depth.</p> <p>Discuss the details of how to create high-quality generative AI prompts to create precise Python PyTorch code.</p> <p>Emphasize the processes around converting text into Knowledge Graphs (KGs) and the use of graph databases and the graph query language Cypher to analyze text.</p> <p>Focus on ways to measure and improve the quality of NLP processes.</p> <p>Assume that the students has some exposure to Python and can read generated Python code.</p> <p>Wrap up with student projects that solve real-world business applications.</p>"},{"location":"prompts/02-modern-course/#introduction-to-modern-nlp","title":"Introduction to Modern NLP","text":"<ul> <li>Course overview and expectations</li> <li>Python and PyTorch refresher</li> <li>The evolution of NLP: From rule-based to neural approaches</li> <li>Brief overview of traditional NLP techniques (parts of speech, stemming, etc.)</li> <li>Introduction to the role of AI tools in modern NLP development</li> </ul>"},{"location":"prompts/02-modern-course/#fundamentals-of-neural-nlp","title":"Fundamentals of Neural NLP","text":"<ul> <li>Vector representations of text</li> <li>Word embeddings (Word2Vec, GloVe, FastText)</li> <li>Neural network architectures for NLP tasks</li> <li>From RNNs to Transformers: Understanding the paradigm shift</li> </ul>"},{"location":"prompts/02-modern-course/#transformer-architecture-deep-dive","title":"Transformer Architecture Deep Dive","text":"<ul> <li>Self-attention mechanisms explained</li> <li>Multi-head attention</li> <li>Positional encoding</li> <li>Layer normalization and feed-forward networks</li> <li>Encoder-decoder architecture</li> </ul>"},{"location":"prompts/02-modern-course/#large-language-models","title":"Large Language Models","text":"<ul> <li>Pre-training and fine-tuning paradigms</li> <li>Transfer learning in NLP</li> <li>Understanding model sizes and capabilities</li> <li>Scaling laws and emergent abilities</li> <li>Architecture variations (decoder-only, encoder-only, encoder-decoder)</li> </ul>"},{"location":"prompts/02-modern-course/#working-with-pre-trained-models","title":"Working with Pre-trained Models","text":"<ul> <li>Using Hugging Face transformers library</li> <li>Interfacing with public LLMs (APIs, local deployment)</li> <li>Parameter-efficient fine-tuning techniques (LoRA, QLoRA, P-tuning)</li> <li>Model quantization and optimization</li> </ul>"},{"location":"prompts/02-modern-course/#prompt-engineering","title":"Prompt Engineering","text":"<ul> <li>Principles of effective prompt design</li> <li>Zero-shot, few-shot, and chain-of-thought prompting</li> <li>Context window management</li> <li>Prompt templates and strategies</li> <li>Generating high-quality PyTorch code with LLMs</li> </ul>"},{"location":"prompts/02-modern-course/#generative-ai-for-code-generation","title":"Generative AI for Code Generation","text":"<ul> <li>Designing prompts specifically for code generation</li> <li>Code quality assessment and improvement</li> <li>Breaking down complex NLP problems for code generation</li> <li>Debugging and refining generated code</li> <li>Creating reusable code generation templates</li> </ul>"},{"location":"prompts/02-modern-course/#text-to-knowledge-graphs","title":"Text to Knowledge Graphs","text":"<ul> <li>Knowledge graph fundamentals</li> <li>Named entity recognition and relation extraction</li> <li>Triple extraction from text</li> <li>Knowledge graph construction techniques</li> <li>Knowledge graph validation and refinement</li> </ul>"},{"location":"prompts/02-modern-course/#graph-databases-and-cypher","title":"Graph Databases and Cypher","text":"<ul> <li>Introduction to graph databases (Neo4j)</li> <li>Cypher query language fundamentals</li> <li>Loading text-derived data into graph databases</li> <li>Graph traversal and pattern matching</li> <li>Complex queries for text analytics</li> </ul>"},{"location":"prompts/02-modern-course/#advanced-knowledge-graph-applications","title":"Advanced Knowledge Graph Applications","text":"<ul> <li>Question answering over knowledge graphs</li> <li>Information retrieval with graph embeddings</li> <li>Combining LLMs and knowledge graphs</li> <li>Reasoning over knowledge structures</li> <li>Graph-augmented language models</li> </ul>"},{"location":"prompts/02-modern-course/#evaluation-metrics-for-nlp","title":"Evaluation Metrics for NLP","text":"<ul> <li>Traditional evaluation metrics vs. modern approaches</li> <li>Human evaluation protocols</li> <li>Automated evaluation frameworks</li> <li>Evaluating factual correctness and hallucinations</li> <li>Benchmarking and comparative analysis</li> </ul>"},{"location":"prompts/02-modern-course/#improving-nlp-quality","title":"Improving NLP Quality","text":"<ul> <li>Addressing biases in NLP systems</li> <li>Retrieval-augmented generation</li> <li>Fact verification techniques</li> <li>Model alignment and safety</li> <li>Handling adversarial inputs</li> </ul>"},{"location":"prompts/02-modern-course/#real-world-nlp-applications","title":"Real-world NLP Applications","text":"<ul> <li>Document processing and analysis</li> <li>Conversational AI systems</li> <li>Information extraction pipelines</li> <li>Multi-modal NLP systems</li> <li>Domain-specific adaptations</li> </ul>"},{"location":"prompts/02-modern-course/#ethical-considerations-in-nlp","title":"Ethical Considerations in NLP","text":"<ul> <li>Privacy concerns with language models</li> <li>Copyright and intellectual property issues</li> <li>Responsible AI development</li> <li>Mitigating harmful outputs</li> <li>Transparency and explainability</li> </ul>"},{"location":"prompts/02-modern-course/#final-projects","title":"Final Projects","text":"<ul> <li>Project ideation and planning</li> <li>Business problem identification</li> <li>Solution architecture design</li> <li>Implementation with PyTorch and LLMs</li> <li>Evaluation and refinement</li> <li>Project presentation and documentation</li> </ul>"},{"location":"prompts/02-modern-course/#advanced-topics-optional","title":"Advanced Topics (Optional)","text":"<ul> <li>Multi-lingual NLP</li> <li>Cross-modal representations</li> <li>Neurosymbolic approaches</li> <li>LLM agents and tool use</li> <li>Latest research directions in NLP</li> </ul>"},{"location":"prompts/03-concept-enumeration/","title":"Core Concepts for Modern NLP Course","text":"<p>Prompt</p> <p>For this Modern NLP course, please create a numbered list the 250 most important concepts in the order they should be taught.  </p> <p>Return numbered list of these core concepts using a single line per concept. </p> <p>For each concept, just return a short concept name and use acronyms or abbreviations when appropriate. They will be used as labels in a graph network. Make sure that the concepts are distinct not duplicated in the list.</p> <p>Use title case for the concept names.</p> <p>Start with the most basic foundational concepts (prerequisites) and then proceed to the most complex concepts.</p> <ol> <li>Programming Fundamentals</li> <li>Python Basics</li> <li>Data Structures</li> <li>Algorithms</li> <li>Functions And Methods</li> <li>Object-Oriented Programming</li> <li>Linear Algebra</li> <li>Probability Theory</li> <li>Statistics Fundamentals</li> <li>Calculus Basics</li> <li>Gradient Descent</li> <li>Backpropagation</li> <li>PyTorch Introduction</li> <li>Tensors</li> <li>Computational Graphs</li> <li>Automatic Differentiation</li> <li>Loss Functions</li> <li>Optimization Algorithms</li> <li>Activation Functions</li> <li>Neural Network Basics</li> <li>Supervised Learning</li> <li>Unsupervised Learning</li> <li>Training And Validation</li> <li>Overfitting And Underfitting</li> <li>Regularization Techniques</li> <li>Hyperparameter Tuning</li> <li>Batch Processing</li> <li>GPU Acceleration</li> <li>Text Representation</li> <li>Tokenization</li> <li>Character-Level Processing</li> <li>Word-Level Processing</li> <li>Subword Tokenization</li> <li>Byte-Pair Encoding (BPE)</li> <li>WordPiece Tokenization</li> <li>SentencePiece Tokenization</li> <li>Text Preprocessing</li> <li>Text Normalization</li> <li>One-Hot Encoding</li> <li>Bag Of Words (BOW)</li> <li>Term Frequency (TF)</li> <li>TF-IDF</li> <li>N-grams</li> <li>Word Embeddings</li> <li>Word2Vec</li> <li>GloVe</li> <li>FastText</li> <li>Contextual Embeddings</li> <li>Neural Network Architectures</li> <li>Feed-Forward Networks</li> <li>Recurrent Neural Networks (RNNs)</li> <li>Long Short-Term Memory (LSTM)</li> <li>Gated Recurrent Units (GRU)</li> <li>Bidirectional RNNs</li> <li>Sequence-to-Sequence Models</li> <li>Encoder-Decoder Architecture</li> <li>Attention Mechanism</li> <li>Self-Attention</li> <li>Multi-Head Attention</li> <li>Transformer Architecture</li> <li>Positional Encoding</li> <li>Layer Normalization</li> <li>Residual Connections</li> <li>Feed-Forward Networks In Transformers</li> <li>Encoder-Only Transformers</li> <li>Decoder-Only Transformers</li> <li>Encoder-Decoder Transformers</li> <li>Pre-Training Strategies</li> <li>Masked Language Modeling (MLM)</li> <li>Next Sentence Prediction (NSP)</li> <li>Causal Language Modeling (CLM)</li> <li>Transfer Learning</li> <li>Fine-Tuning</li> <li>Parameter-Efficient Fine-Tuning</li> <li>Low-Rank Adaptation (LoRA)</li> <li>QLoRA</li> <li>Adapter Layers</li> <li>Prompt Tuning</li> <li>P-Tuning</li> <li>Large Language Models (LLMs)</li> <li>BERT</li> <li>RoBERTa</li> <li>GPT Architecture</li> <li>T5 Architecture</li> <li>BART</li> <li>PaLM</li> <li>LLaMA</li> <li>Model Scaling Laws</li> <li>Emergent Abilities</li> <li>Model Distillation</li> <li>Model Quantization</li> <li>Weight Pruning</li> <li>Model Compression</li> <li>Hugging Face Transformers Library</li> <li>Model Evaluation</li> <li>Perplexity</li> <li>BLEU Score</li> <li>ROUGE Score</li> <li>F1 Score</li> <li>Precision And Recall</li> <li>Human Evaluation</li> <li>Prompt Engineering</li> <li>Zero-Shot Learning</li> <li>Few-Shot Learning</li> <li>Chain-Of-Thought Prompting</li> <li>In-Context Learning</li> <li>Prompt Templates</li> <li>Context Window Management</li> <li>System Prompts</li> <li>User Prompts</li> <li>Prompt Chaining</li> <li>ReAct Framework</li> <li>Code Generation Prompts</li> <li>Code Quality Assessment</li> <li>Named Entity Recognition (NER)</li> <li>Part-Of-Speech (POS) Tagging</li> <li>Dependency Parsing</li> <li>Constituency Parsing</li> <li>Semantic Role Labeling</li> <li>Sentiment Analysis</li> <li>Text Classification</li> <li>Text Summarization</li> <li>Extractive Summarization</li> <li>Abstractive Summarization</li> <li>Machine Translation</li> <li>Question Answering</li> <li>Open-Domain QA</li> <li>Closed-Domain QA</li> <li>Information Retrieval</li> <li>Semantic Search</li> <li>Natural Language Inference (NLI)</li> <li>Textual Entailment</li> <li>Coreference Resolution</li> <li>Entity Linking</li> <li>Relation Extraction</li> <li>Triple Extraction</li> <li>Knowledge Graphs (KGs)</li> <li>Graph Theory Basics</li> <li>Knowledge Representation</li> <li>Ontologies</li> <li>RDF Triples</li> <li>Knowledge Base Construction</li> <li>Graph Databases</li> <li>Neo4j</li> <li>Cypher Query Language</li> <li>Graph Patterns</li> <li>Node Properties</li> <li>Edge Properties</li> <li>Graph Traversal</li> <li>Path Finding</li> <li>Graph Embeddings</li> <li>Knowledge Graph Completion</li> <li>Link Prediction</li> <li>Knowledge Graph Reasoning</li> <li>Graph Neural Networks (GNNs)</li> <li>Graph Convolutional Networks (GCNs)</li> <li>Graph Attention Networks (GATs)</li> <li>Knowledge Graph Validation</li> <li>Query Optimization</li> <li>Retrieval-Augmented Generation (RAG)</li> <li>Vector Databases</li> <li>Embeddings Storage</li> <li>Similarity Search</li> <li>ANN Algorithms</li> <li>Document Chunking</li> <li>Context Retrieval</li> <li>Fact Verification</li> <li>Hallucination Detection</li> <li>Factual Consistency</li> <li>LLM Evaluation Frameworks</li> <li>Bias In NLP</li> <li>Bias Mitigation</li> <li>Fairness Metrics</li> <li>Robustness Testing</li> <li>Adversarial Examples</li> <li>Model Alignment</li> <li>Reinforcement Learning From Human Feedback (RLHF)</li> <li>Constitutional AI</li> <li>RLHF Reward Models</li> <li>Direct Preference Optimization (DPO)</li> <li>Model Interpretability</li> <li>Attention Visualization</li> <li>Feature Attribution</li> <li>Explainability Methods</li> <li>Domain Adaptation</li> <li>Multi-Domain Learning</li> <li>Multi-Lingual NLP</li> <li>Cross-Lingual Transfer</li> <li>Language-Agnostic Representations</li> <li>Low-Resource Languages</li> <li>Multi-Modal NLP</li> <li>Vision-Language Models</li> <li>Text-To-Image Generation</li> <li>Image-To-Text Generation</li> <li>Audio-Text Models</li> <li>Speech Recognition</li> <li>Text-To-Speech</li> <li>Document Understanding</li> <li>Layout Analysis</li> <li>OCR Integration</li> <li>Table Extraction</li> <li>Conversational AI</li> <li>Dialogue Management</li> <li>Response Generation</li> <li>Intent Recognition</li> <li>Slot Filling</li> <li>Dialogue State Tracking</li> <li>Ethical Considerations</li> <li>Privacy Concerns</li> <li>Copyright Issues</li> <li>Model Attribution</li> <li>Responsible AI Development</li> <li>Safety Guardrails</li> <li>Content Filtering</li> <li>Neurosymbolic Approaches</li> <li>Reasoning Over Knowledge</li> <li>Logical Reasoning</li> <li>Probabilistic Reasoning</li> <li>LLM Agents</li> <li>Tool Use</li> <li>Function Calling</li> <li>API Integration</li> <li>Agentic Frameworks</li> <li>Autonomous Systems</li> <li>LLM Orchestration</li> <li>Evaluation Pipelines</li> <li>Performance Monitoring</li> <li>A/B Testing</li> <li>User Feedback Collection</li> <li>Continuous Improvement</li> <li>Production Deployment</li> <li>Scalability Considerations</li> <li>Latency Optimization</li> <li>Batch Processing Vs. Real-Time</li> <li>Stream Processing</li> <li>Distributed Computing</li> <li>Container Orchestration</li> <li>MLOps For NLP</li> <li>CI/CD For NLP Models</li> <li>Version Control For Models</li> <li>Model Serving</li> <li>Model Monitoring</li> <li>Real-World Applications</li> <li>Business Use Cases</li> <li>ROI Calculation</li> <li>System Design</li> <li>Integration Patterns</li> <li>Solution Architecture</li> <li>Project Management</li> <li>Project Documentation</li> </ol>"},{"location":"prompts/30-doc-classification/","title":"Document Classification Architecture","text":"<p>Prompt</p> <p>What architecture would you recommend for this application?  Please specify what data you would need for input such as known documents with classification labels as well as training process for a classification model as well as what types of ways we can measure the success of these classification models.</p> <p>I'll provide a recommended architecture for a document classification system, including required data inputs, training process, and evaluation metrics.</p>"},{"location":"prompts/30-doc-classification/#document-classification-system-architecture","title":"Document Classification System Architecture","text":""},{"location":"prompts/30-doc-classification/#system-architecture-overview","title":"System Architecture Overview","text":"<p>I recommend a modular architecture with the following components:</p> <ol> <li>Data Ingestion Pipeline</li> <li>Preprocessing Module</li> <li>Feature Extraction Module</li> <li>Classification Model</li> <li>Evaluation Framework</li> <li>Inference Engine</li> <li>User Interface/API</li> </ol>"},{"location":"prompts/30-doc-classification/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>+-------------------+     +------------------+     +-----------------+\n| Data Ingestion    |----&gt;| Preprocessing    |----&gt;| Feature         |\n| Pipeline          |     | Module           |     | Extraction      |\n+-------------------+     +------------------+     +-----------------+\n                                                          |\n                                                          v\n+------------------+     +------------------+     +-----------------+\n| User Interface/  |&lt;----| Inference        |&lt;----| Classification  |\n| API              |     | Engine           |     | Model           |\n+------------------+     +------------------+     +-----------------+\n                                                          ^\n                                                          |\n                                                  +-----------------+\n                                                  | Evaluation      |\n                                                  | Framework       |\n                                                  +-----------------+\n</code></pre>"},{"location":"prompts/30-doc-classification/#required-data-inputs","title":"Required Data Inputs","text":""},{"location":"prompts/30-doc-classification/#labeled-document-dataset","title":"Labeled Document Dataset","text":"<ul> <li>Training Set: A substantial collection of documents with known classification labels</li> <li>Validation Set: Separate labeled documents for tuning hyperparameters</li> <li>Test Set: Holdout labeled documents for final evaluation</li> </ul>"},{"location":"prompts/30-doc-classification/#dataset-characteristics","title":"Dataset Characteristics","text":"<ul> <li>Balanced representation across all target classes</li> <li>Sufficient samples per class (minimum 100 documents per class)</li> <li>Documents similar to those expected in production</li> <li>Coverage of edge cases and ambiguous examples</li> </ul>"},{"location":"prompts/30-doc-classification/#training-process","title":"Training Process","text":""},{"location":"prompts/30-doc-classification/#1-data-preprocessing","title":"1. Data Preprocessing","text":"<ul> <li>Text cleaning (removing HTML, special characters)</li> <li>Tokenization</li> <li>Stopword removal (optional)</li> <li>Lemmatization/stemming (optional)</li> <li>Handling of multilingual content if applicable</li> </ul>"},{"location":"prompts/30-doc-classification/#2-feature-extraction","title":"2. Feature Extraction","text":"<p>Traditional Approaches:</p> <ul> <li>TF-IDF vectorization</li> <li>N-gram extraction</li> <li>Topic modeling features (LDA)</li> </ul> <p>Modern Approaches:</p> <ul> <li>Pretrained embeddings (Word2Vec, GloVe)</li> <li>Contextual embeddings (BERT, RoBERTa, etc.)</li> <li>Document-level embeddings</li> </ul>"},{"location":"prompts/30-doc-classification/#3-model-selection","title":"3. Model Selection","text":"<p>Traditional Models:</p> <ul> <li>Logistic Regression</li> <li>Support Vector Machines</li> <li>Random Forests</li> </ul> <p>Deep Learning Models:</p> <ul> <li>Convolutional Neural Networks</li> <li>Recurrent Neural Networks (LSTM/GRU)</li> <li>Transformer-based models (fine-tuned BERT, etc.)</li> </ul>"},{"location":"prompts/30-doc-classification/#4-training-pipeline","title":"4. Training Pipeline","text":"<ul> <li>Cross-validation for hyperparameter tuning</li> <li>Learning rate scheduling</li> <li>Early stopping</li> <li>Model checkpointing</li> <li>Regularization techniques</li> </ul>"},{"location":"prompts/30-doc-classification/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"prompts/30-doc-classification/#classification-performance","title":"Classification Performance","text":"<ul> <li>Accuracy: Overall correctness (suitable for balanced datasets)</li> <li>Precision: Correctness of positive predictions (per class)</li> <li>Recall: Completeness of positive predictions (per class)</li> <li>F1 Score: Harmonic mean of precision and recall</li> <li>Macro/Micro/Weighted Averages: Aggregated metrics across classes</li> </ul>"},{"location":"prompts/30-doc-classification/#confusion-matrix-analysis","title":"Confusion Matrix Analysis","text":"<ul> <li>Identifying commonly confused classes</li> <li>Class-specific performance analysis</li> </ul>"},{"location":"prompts/30-doc-classification/#advanced-evaluation","title":"Advanced Evaluation","text":"<ul> <li>ROC Curves and AUC: For binary classification tasks</li> <li>Precision-Recall Curves: Especially for imbalanced datasets</li> <li>Calibration Curves: Assessing probability estimates</li> </ul>"},{"location":"prompts/30-doc-classification/#production-oriented-metrics","title":"Production-Oriented Metrics","text":"<ul> <li>Inference Time: Processing time per document</li> <li>Memory Usage: Resource requirements</li> <li>Scalability: Performance under high load</li> <li>Throughput: Documents processed per time unit</li> </ul>"},{"location":"prompts/30-doc-classification/#additional-considerations","title":"Additional Considerations","text":""},{"location":"prompts/30-doc-classification/#model-explainability","title":"Model Explainability","text":"<ul> <li>Feature importance analysis</li> <li>LIME or SHAP for local explanations</li> <li>Attention visualization for transformer models</li> </ul>"},{"location":"prompts/30-doc-classification/#deployment-strategy","title":"Deployment Strategy","text":"<ul> <li>Containerization (Docker)</li> <li>Model versioning and experiment tracking</li> <li>A/B testing framework</li> <li>Monitoring for model drift</li> </ul>"},{"location":"prompts/30-doc-classification/#continuous-improvement","title":"Continuous Improvement","text":"<ul> <li>Regular retraining schedule</li> <li>Active learning for efficient labeling of new data</li> <li>Feedback loop from production predictions</li> </ul> <p>This architecture provides a comprehensive framework for building a robust document classification system that can be evaluated effectively and deployed with confidence.</p>"},{"location":"prompts/31-doc-classification-code/","title":"Document Classification Code","text":"<p>Prompt</p> <p>Please generate some sample Python code that would read two folders of documents, one folder that is positive for the selection criteria and one folder that is negative.  One you read the documents, use them as a training set to build a predictive model.  Use the state-of-the-art Python machine learning models that predict the classification of documents.</p> <pre><code>import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset\n\n# Function to read documents from folders\ndef read_documents(positive_folder, negative_folder):\n    documents = []\n    labels = []\n\n    # Read positive documents\n    for filename in os.listdir(positive_folder):\n        if filename.endswith('.txt'):\n            file_path = os.path.join(positive_folder, filename)\n            with open(file_path, 'r', encoding='utf-8') as file:\n                content = file.read()\n                documents.append(content)\n                labels.append(1)  # Positive class\n\n    # Read negative documents\n    for filename in os.listdir(negative_folder):\n        if filename.endswith('.txt'):\n            file_path = os.path.join(negative_folder, filename)\n            with open(file_path, 'r', encoding='utf-8') as file:\n                content = file.read()\n                documents.append(content)\n                labels.append(0)  # Negative class\n\n    return documents, labels\n\n# Main function\ndef main():\n    # Define paths to document folders\n    positive_folder = \"./positive_documents\"\n    negative_folder = \"./negative_documents\"\n\n    # Read documents\n    print(\"Reading documents...\")\n    documents, labels = read_documents(positive_folder, negative_folder)\n\n    # Create a dataframe\n    df = pd.DataFrame({\n        'text': documents,\n        'label': labels\n    })\n\n    # Print dataset statistics\n    print(f\"Total documents: {len(df)}\")\n    print(f\"Positive documents: {sum(df['label'] == 1)}\")\n    print(f\"Negative documents: {sum(df['label'] == 0)}\")\n\n    # Split the dataset into training and testing sets\n    train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n\n    print(f\"Training set size: {len(train_df)}\")\n    print(f\"Testing set size: {len(test_df)}\")\n\n    # Initialize BERT tokenizer and model\n    model_name = \"roberta-base\"  # You can use other models like \"bert-base-uncased\", \"distilbert-base-uncased\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\n    # Tokenize the text\n    def tokenize_function(examples):\n        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n\n    # Prepare datasets\n    train_dataset = Dataset.from_pandas(train_df)\n    test_dataset = Dataset.from_pandas(test_df)\n\n    # Tokenize datasets\n    train_dataset = train_dataset.map(tokenize_function, batched=True)\n    test_dataset = test_dataset.map(tokenize_function, batched=True)\n\n    # Set up training arguments\n    training_args = TrainingArguments(\n        output_dir=\"./results\",\n        num_train_epochs=3,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        warmup_steps=500,\n        weight_decay=0.01,\n        logging_dir=\"./logs\",\n        logging_steps=10,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n    )\n\n    # Define trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n    )\n\n    # Train the model\n    print(\"Training the model...\")\n    trainer.train()\n\n    # Evaluate the model\n    print(\"Evaluating the model...\")\n    eval_result = trainer.evaluate()\n    print(f\"Evaluation results: {eval_result}\")\n\n    # Make predictions on test set\n    predictions = trainer.predict(test_dataset)\n    preds = np.argmax(predictions.predictions, axis=1)\n\n    # Print classification report\n    print(\"Classification Report:\")\n    print(classification_report(test_df['label'], preds))\n\n    # Print confusion matrix\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(test_df['label'], preds))\n\n    # Save the model\n    trainer.save_model(\"./saved_model\")\n    tokenizer.save_pretrained(\"./saved_model\")\n    print(\"Model saved to ./saved_model\")\n\n    # Function to predict on new documents\n    def predict_document(document_text):\n        inputs = tokenizer(document_text, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n        outputs = model(**inputs)\n        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n        pred_class = torch.argmax(probs, dim=1).item()\n        confidence = probs[0][pred_class].item()\n        return pred_class, confidence\n\n    # Example usage of prediction function\n    print(\"\\nExample prediction:\")\n    example_doc = \"This is an example document for prediction.\"\n    pred_class, confidence = predict_document(example_doc)\n    print(f\"Predicted class: {'Positive' if pred_class == 1 else 'Negative'}\")\n    print(f\"Confidence: {confidence:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"}]}